{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 环境配置\n",
    "\n",
    "http://tangshusen.me/Dive-into-DL-PyTorch/#/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 anaconda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conda 的常用命令\n",
    "\n",
    "- conda create -name env_name # 创建新的环境\n",
    "- conda activate env_name # 激活环境\n",
    "- conda deactivate # 退出环境\n",
    "- conda remove env_name --all # 删除环境\n",
    "- \n",
    "- conda list # 查看所有已安装的包\n",
    "- conda install package_name # 当前环境下安装包\n",
    "- conda remove package_name # 删除当前环境下包\n",
    "- conda install -name env_name package_name # 指定环境下安装包\n",
    "- conda remove -name env_name package_name # 删除指定环境下安装包\n",
    "\n",
    "更新 conda 安装源\n",
    "\n",
    "```\n",
    "conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\n",
    "conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/\n",
    "conda config --set show_channel_urls yes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### jupyter 是什么\n",
    "\n",
    "Jupyter notebook（http://jupyter.org/） 是一种 Web 应用，能让用户将说明文本、数学方程、代码和可视化内容全部组合到一个易于共享的文档中。\n",
    "\n",
    "#### 安装使用\n",
    "\n",
    "1. 安装 anaconda\n",
    "2. 命令行输入 jupyter note\n",
    "3. 浏览器打开 http://localhost:8888 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### jupyter 数学公式\n",
    "\n",
    "| 符号 | 示例 | 样式\n",
    "| :-- | :-- |:-- |\n",
    "| \\$ | ```$y = ax + bx +c$``` | $y = ax + bx + c $\n",
    "| \\$\\$ | ```$$y = ax + bx + c $$``` | $$y = ax + bx + c $$\n",
    "| _ | ```x_1``` | $x_1$\n",
    "| ^ |``` x^1``` | $x^1$\n",
    "| \\sum | ```\\sum_{i=0}^{n}(x_i^2 + y_j^3)``` | $$\\sum_{i=0}^{n}(x_i^2 + y_j^3)$$\n",
    "| \\frac{分子}{分母} | ```\\frac{1}{3} - \\frac{x_i^2}{y_j^3```} | $\\cfrac{1}{3} - \\frac{x_i^2}{y_j^3}$\n",
    "|\\left[ \\right]  | ```\\left[ \\begin{array} {cccc} X_1&Y_1^2 X_2 & Y_2^2 \\ldots X_n&Y_n^2 \\end{array} \\right]``` | $$\\left[ \\begin{array} {cccc} X_1&Y_1^2 \\\\X_2 & Y_2^2 \\\\\\ldots \\\\X_n&Y_n^2 \\end{array} \\right] $$\n",
    "| \\sqrt[次数]{被开方数} | \\sqrt[2]{b^2-4ac} | $$ \\sqrt[2]{b^2-4ac}$$\n",
    "| \\ldots |  f(x_1,x_2,\\ldots,x_i) | $ f(x_1,x_2,\\ldots,x_i) $\n",
    "| \\cdots | x_1+x_2+\\cdots + x_i | $x_1+x_2+\\cdots + x_i $\n",
    "| \\\\{ \\\\} | \\\\{ \\\\} | $\\{ \\}$\n",
    "|\\left( \\right| \\left(y^2 + \\frac{x^2+z}{1+z^2}\\right) | $ \\left(y^2 + \\frac{x^2+z}{1+z^2}\\right)$\n",
    "|\\begin \\end | ```\\begin{eqnarray*} \\cos 2 \\theta & = & \\cos^2 \\theta - \\sin^2 \\theta\\\\ &=& 2\\cos^2 \\theta - 1 \\end{eqnarray*}``` | $ \\begin{eqnarray*} \\cos 2 \\theta & = & \\cos^2 \\theta - \\sin^2 \\theta\\\\ &=& 2\\cos^2 \\theta - 1 \\end{eqnarray*}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 安装 gpu 版\n",
    " conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch\n",
    " \n",
    " 安装 cpu 版\n",
    " conda install pytorch torchvision torchaudio cpuonly -c pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 数据操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 创建 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.5137e+22,  4.5827e-41, -3.5137e+22],\n",
       "        [ 4.5827e-41, -3.5137e+22,  4.5827e-41],\n",
       "        [-3.4836e+22,  4.5827e-41, -3.5138e+22],\n",
       "        [ 4.5827e-41, -3.4836e+22,  4.5827e-41],\n",
       "        [-3.5141e+22,  4.5827e-41, -3.5141e+22]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# 创建 初始 5x3 矩阵\n",
    "x = torch.empty(5, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4482, 0.9617, 0.6117],\n",
       "        [0.0042, 0.8376, 0.1709],\n",
       "        [0.0384, 0.5384, 0.6804],\n",
       "        [0.2953, 0.6850, 0.7126],\n",
       "        [0.6319, 0.5372, 0.7965]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# 创建 随机 5x3 矩阵\n",
    "x = torch.rand(5, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# 创建 long 型 5x3 零矩形\n",
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.0100, 6.0000])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([5.01, 6.])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[-0.7359, -0.0727,  0.0873],\n",
      "        [ 2.2633,  2.1353, -0.4354],\n",
      "        [ 1.9927, -1.1116,  1.8314],\n",
      "        [ 0.5284, -0.4493, -0.1776],\n",
      "        [ 0.1367,  0.5788, -0.8017]])\n",
      "torch.Size([5, 3])\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "# 返回的tensor默认具有相同的torch.dtype和torch.device\n",
    "x = x.new_ones(5, 3, dtype=torch.float64)  \n",
    "print(x)\n",
    "\n",
    "# 指定新的数据类型\n",
    "x = torch.randn_like(x, dtype=torch.float) \n",
    "print(x) \n",
    "\n",
    "# 获取数据形状\n",
    "print(x.size())\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "常用 函数\n",
    "\n",
    "| 函数 |功能\n",
    "| :-- | :--\n",
    "| Tensor(*sizes) | 基础构造函数\n",
    "| tensor(data,) | 类似np.array的构造函数\n",
    "| ones(*sizes)\t| 全1Tensor\n",
    "| zeros(*sizes)\t| 全0Tensor\n",
    "| eye(*sizes)\t| 对角线为1，其他为0\n",
    "| arange(s,e,step) | 从s到e，步长为step\n",
    "| linspace(s,e,steps) | 从s到e，均匀切分成steps份\n",
    "| rand/randn(*sizes) | 均匀/标准分布\n",
    "| normal(mean,std)/uniform(from,to) | 正态分布/均匀分布\n",
    "| randperm(m) | 随机排列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 操作 tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 算术操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.4629, 1.0321, 0.9401],\n",
      "        [1.2077, 0.6158, 1.0661],\n",
      "        [0.6125, 1.4500, 1.3571],\n",
      "        [1.1284, 0.2369, 1.2080],\n",
      "        [1.0073, 0.4616, 0.9221]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "y = torch.rand(5, 3)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4922, 1.1587, 0.5226],\n",
      "        [1.5170, 0.5472, 1.3716],\n",
      "        [1.3573, 0.6225, 1.5440],\n",
      "        [1.5604, 1.2344, 1.7498],\n",
      "        [1.0899, 1.1420, 0.9120]])\n"
     ]
    }
   ],
   "source": [
    "result = torch.empty(5, 3)\n",
    "x = torch.rand(5, 3)\n",
    "y = torch.rand(5, 3)\n",
    "torch.add(x, y, out = result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4922, 1.1587, 0.5226],\n",
      "        [1.5170, 0.5472, 1.3716],\n",
      "        [1.3573, 0.6225, 1.5440],\n",
      "        [1.5604, 1.2344, 1.7498],\n",
      "        [1.0899, 1.1420, 0.9120]])\n"
     ]
    }
   ],
   "source": [
    "y = y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2471, 0.3275, 0.0640])\n"
     ]
    }
   ],
   "source": [
    "y = x[0, :]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 函数 | 功能\n",
    "| :-- | :--|\n",
    "| index_select(input, dim, index) | 在指定维度dim上选取，比如选取某些行、某些列\n",
    "| masked_select(input, mask) | 例子如上，a[a>0]，使用ByteTensor进行选取\n",
    "| nonzero(input) | 非0元素的下标\n",
    "| gather(input, dim, index) | 根据index，在dim维度上选取数据，输出的size与index一样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 改变形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2471, 0.3275, 0.0640, 0.9543, 0.1112, 0.4998, 0.7489, 0.3966, 0.6445,\n",
      "        0.8969, 0.5084, 0.7728, 0.2252, 0.5354, 0.8450])\n"
     ]
    }
   ],
   "source": [
    "# view 改变形状，也共享内存\n",
    "y = x.view(15)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2471, 0.3275, 0.0640, 0.9543, 0.1112, 0.4998, 0.7489, 0.3966, 0.6445,\n",
      "        0.8969, 0.5084, 0.7728, 0.2252, 0.5354, 0.8450])\n"
     ]
    }
   ],
   "source": [
    "# 使用 clone() 创建新副本\n",
    "z = x.clone().view(15)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07952862977981567\n"
     ]
    }
   ],
   "source": [
    "# tensor -> 数值\n",
    "z = torch.rand(1)\n",
    "print(z.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 线性代数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 函数 | 功能\n",
    "| :-- | :-- |\n",
    "| trace | 对角线元素之和(矩阵的迹)\n",
    "| diag | 对角线元素\n",
    "| triu/tril | 矩阵的上三角/下三角，可指定偏移量\n",
    "| mm/bmm | 矩阵乘法，batch的矩阵乘法\n",
    "| addmm/addbmm/addmv/addr/baddbmm..| 矩阵运算\n",
    "| t | 转置\n",
    "| dot/cross | 内积/外积\n",
    "| inverse | 求逆矩阵\n",
    "| svd | 奇异值分解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 广播机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当对两个形状不同的Tensor按元素运算时，可能会触发广播（broadcasting）机制：先适当复制元素使这两个Tensor形状相同后再按元素运算。例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[2, 3],\n",
      "        [3, 4],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1, 3).view(1, 2)\n",
    "print(x)\n",
    "y = torch.arange(1, 4).view(3, 1)\n",
    "print(y)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 运算的内存开销"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5 Tensor 和 Numpy 的互换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "# Tensor -> Numpy\n",
    "b = a.numpy()\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "# numpy -> tensor\n",
    "b = torch.from_numpy(a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.6 Tensor On GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.5137e+22,  1.0000e+00, -3.5137e+22],\n",
      "        [ 1.0000e+00, -3.5137e+22,  1.0000e+00],\n",
      "        [-3.4836e+22,  1.0000e+00, -3.5138e+22],\n",
      "        [ 1.0000e+00, -3.4836e+22,  1.0000e+00],\n",
      "        [-3.5141e+22,  1.0000e+00, -3.5141e+22]], device='cuda:0')\n",
      "tensor([[-3.5137e+22,  1.0000e+00, -3.5137e+22],\n",
      "        [ 1.0000e+00, -3.5137e+22,  1.0000e+00],\n",
      "        [-3.4836e+22,  1.0000e+00, -3.5138e+22],\n",
      "        [ 1.0000e+00, -3.4836e+22,  1.0000e+00],\n",
      "        [-3.5141e+22,  1.0000e+00, -3.5141e+22]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 以下代码只有在PyTorch GPU版本上才会执行\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # GPU\n",
    "    y = torch.ones_like(x, device=device)  # 直接创建一个在GPU上的Tensor\n",
    "    x = x.to(device)                       # 等价于 .to(\"cuda\")\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # to()还可以同时更改数据类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 自动求梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 概念\n",
    "\n",
    "1. 求梯度\n",
    "\n",
    "- 将 Tensor 属性 .requires_grad 设置为True，将开始追踪(track)在其上的所有操作（这样就可以利用链式法则进行梯度传播了）\n",
    "- 调用 .backward() 完成所有梯度计算， Tensor 的梯度将累积到 .grad 属性中\n",
    "\n",
    "2. 取消梯度\n",
    "\n",
    "- 调用 .detach(), 取消跟踪，这样梯度就传不过去了\n",
    "- 可用 with torch.no_grad() 将不想被追踪的操作代码块包裹起来，这种方法在评估模型的时候很常用。\n",
    "- 也可 设置 requires_grad = False 取消梯度。\n",
    "\n",
    "- Tensor 和 Function 互相结合就可构建一个记录有整个计算过程的有向无环图（DAG）。\n",
    "- Tensor 有一个 .grad_fn 属性，该属性即创建该 Tensor 的 Function。\n",
    "- 就是说该 Tensor 是不是通过某些运算得到的，若是，则 grad_fn 返回一个与这些运算相关的对象，否则是 None。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 设置 requires_grad\n",
    "x = torch.ones(2, 2, requires_grad = True)\n",
    "print(x)\n",
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "None\n",
      "<AddBackward0 object at 0x7fbef65247c0>\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)\n",
    "print(x.grad)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意： 在 y.backward() 时，如果 y 是标量，则不需要为 backward() 传入任何参数；否则，需要传入一个与 y 同形的 Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "y = x + 2\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "# 求梯度\n",
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor(1., grad_fn=<PowBackward0>) True\n",
      "tensor(1.) False\n",
      "tensor(2., grad_fn=<AddBackward0>) True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y1 = x ** 2\n",
    "# 取消求梯度\n",
    "with torch.no_grad():\n",
    "    y2 = x ** 3\n",
    "y3 = y1 + y2\n",
    "\n",
    "print(x.requires_grad)\n",
    "print(y1, y1.requires_grad) # True\n",
    "print(y2, y2.requires_grad) # False\n",
    "print(y3, y3.requires_grad) # True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们想要修改 tensor 的数值，但是又不希望被 autograd 记录（即不会影响反向传播），那么我么可以对 tensor.data 进行操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n",
      "False\n",
      "tensor([100.], requires_grad=True)\n",
      "tensor([200.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1, requires_grad=True)\n",
    "\n",
    "print(x.data) # 还是一个tensor\n",
    "print(x.data.requires_grad) # 但是已经是独立于计算图之外\n",
    "\n",
    "y = x * x\n",
    "x.data *= 100 # 只改变了值，不会记录在计算图，所以不会影响梯度传播\n",
    "\n",
    "y.backward()\n",
    "print(x) # 更改data的值也会影响tensor的值\n",
    "print(x.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
