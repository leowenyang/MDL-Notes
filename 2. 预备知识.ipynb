{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 环境配置\n",
    "\n",
    "http://tangshusen.me/Dive-into-DL-PyTorch/#/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 anaconda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### anaconda 是什么？\n",
    "\n",
    "Anaconda是Python的一个开源发行版本，主要面向科学计算。我们可以简单理解为，Anaconda是一个预装了很多我们用的到或用不到的第三方库的Python。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### anaconda 安装\n",
    "\n",
    "- 官网下载软件\n",
    "- 安装软件\n",
    "- 测试安装是否成功\n",
    "```\n",
    "$ python\n",
    "$ ipython\n",
    "$ conda\n",
    "$ jupyter notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### anaconda 环境及软件包管理 \n",
    "\n",
    "1. 环境\n",
    "\n",
    "- conda create -name env_name            # 创建新的环境\n",
    "- conda activate env_name               # 激活环境\n",
    "- conda deactivate                   # 退出环境\n",
    "- conda remove env_name --all            # 删除环境\n",
    "\n",
    "2. 软件包\n",
    "\n",
    "- conda list                        # 查看所有已安装的包\n",
    "- conda install package_name             # 当前环境下安装包\n",
    "- conda remove package_name              # 删除当前环境下包\n",
    "- conda install -name env_name package_name   # 指定环境下安装包\n",
    "- conda remove -name env_name package_name    # 删除指定环境下安装包"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 常见问题\n",
    "\n",
    "1. 下载 anaconda 的速度太慢，怎么办？\n",
    "\n",
    "使用国内清华大学提供的镜像站进行下载。 镜像站的地址为 https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/\n",
    "\n",
    "2. 使用 conda 命令安装软件太慢，怎么办？\n",
    "\n",
    "更换 conda 的安装源，使用清华大学的镜像源，操作如下：\n",
    "\n",
    "```\n",
    "conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\n",
    "conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/\n",
    "conda config --set show_channel_urls yes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### jupyter 是什么\n",
    "\n",
    "Jupyter notebook（http://jupyter.org/） 是一种 Web 应用，能让用户将说明文本、数学方程、代码和可视化内容全部组合到一个易于共享的文档中。\n",
    "\n",
    "#### 安装使用\n",
    "\n",
    "1. 安装 anaconda\n",
    "2. 命令行输入 jupyter note\n",
    "3. 浏览器打开 http://localhost:8888 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### jupyter 数学公式\n",
    "\n",
    "| 符号 | 示例 | 样式\n",
    "| :-- | :-- |:-- |\n",
    "| \\$ | ```$y = ax + bx +c$``` | $y = ax + bx + c $\n",
    "| \\$\\$ | ```$$y = ax + bx + c $$``` | $$y = ax + bx + c $$\n",
    "| _ | ```x_1``` | $x_1$\n",
    "| ^ |``` x^1``` | $x^1$\n",
    "| \\sum | ```\\sum_{i=0}^{n}(x_i^2 + y_j^3)``` | $$\\sum_{i=0}^{n}(x_i^2 + y_j^3)$$\n",
    "| \\frac{分子}{分母} | ```\\frac{1}{3} - \\frac{x_i^2}{y_j^3```} | $\\cfrac{1}{3} - \\frac{x_i^2}{y_j^3}$\n",
    "|\\left[ \\right]  | ```\\left[ \\begin{array} {cccc} X_1&Y_1^2 X_2 & Y_2^2 \\ldots X_n&Y_n^2 \\end{array} \\right]``` | $$\\left[ \\begin{array} {cccc} X_1&Y_1^2 \\\\X_2 & Y_2^2 \\\\\\ldots \\\\X_n&Y_n^2 \\end{array} \\right] $$\n",
    "| \\sqrt[次数]{被开方数} | \\sqrt[2]{b^2-4ac} | $$ \\sqrt[2]{b^2-4ac}$$\n",
    "| \\ldots |  f(x_1,x_2,\\ldots,x_i) | $ f(x_1,x_2,\\ldots,x_i) $\n",
    "| \\cdots | x_1+x_2+\\cdots + x_i | $x_1+x_2+\\cdots + x_i $\n",
    "| \\\\{ \\\\} | \\\\{ \\\\} | $\\{ \\}$\n",
    "|\\left( \\right| \\left(y^2 + \\frac{x^2+z}{1+z^2}\\right) | $ \\left(y^2 + \\frac{x^2+z}{1+z^2}\\right)$\n",
    "|\\begin \\end | ```\\begin{eqnarray*} \\cos 2 \\theta & = & \\cos^2 \\theta - \\sin^2 \\theta\\\\ &=& 2\\cos^2 \\theta - 1 \\end{eqnarray*}``` | $ \\begin{eqnarray*} \\cos 2 \\theta & = & \\cos^2 \\theta - \\sin^2 \\theta\\\\ &=& 2\\cos^2 \\theta - 1 \\end{eqnarray*}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 安装 gpu 版\n",
    " conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch\n",
    " \n",
    " 安装 cpu 版\n",
    " conda install pytorch torchvision torchaudio cpuonly -c pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 数据操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 创建 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.9003e+25,  4.5633e-41, -2.9003e+25],\n",
      "        [ 4.5633e-41,  4.4842e-44,  0.0000e+00],\n",
      "        [ 1.5695e-43,  0.0000e+00, -2.5195e-07],\n",
      "        [ 3.0796e-41,  8.9683e-44,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
      "tensor([[0.8317, 0.6471, 0.2042],\n",
      "        [0.1685, 0.5975, 0.9520],\n",
      "        [0.1693, 0.1278, 0.7219],\n",
      "        [0.2051, 0.1014, 0.9200],\n",
      "        [0.9609, 0.0980, 0.1648]])\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "tensor([5.0100, 6.0000])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[ 2.5233,  0.8439,  0.4362],\n",
      "        [-0.2117, -0.6866, -0.7341],\n",
      "        [ 1.9516, -0.4451,  1.9719],\n",
      "        [ 0.5170,  1.5705,  0.3910],\n",
      "        [-0.2368, -0.7552, -1.3680]])\n",
      "torch.Size([5, 3])\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 创建 初始 5x3 矩阵\n",
    "x = torch.empty(5, 3)\n",
    "print(x)\n",
    "# 创建 随机 5x3 矩阵\n",
    "x = torch.rand(5, 3)\n",
    "print(x)\n",
    "# 创建 long 型 5x3 零矩形\n",
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "print(x)\n",
    "# 根据数据创建 Tensor\n",
    "x = torch.tensor([5.01, 6.])\n",
    "print(x)\n",
    "# 返回的tensor默认具有相同的 torch.dtype和torch.device\n",
    "x = x.new_ones(5, 3, dtype=torch.float64)  \n",
    "print(x)\n",
    "# 指定新的数据类型\n",
    "x = torch.randn_like(x, dtype=torch.float) \n",
    "print(x) \n",
    "# 获取数据形状\n",
    "print(x.size())\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**常用 函数**\n",
    "\n",
    "| 函数 |功能\n",
    "| :-- | :--\n",
    "| Tensor(*sizes) | 基础构造函数\n",
    "| tensor(data,) | 类似 np.array 的构造函数\n",
    "| ones(*sizes)\t| 全 1 Tensor\n",
    "| zeros(*sizes)\t| 全 0 Tensor\n",
    "| eye(*sizes)\t| 对角线为 1，其他为 0\n",
    "| arange(s,e,step) | 从 s 到 e，步长为 step\n",
    "| linspace(s,e,steps) | 从 s 到 e，均匀切分成 steps 份\n",
    "| rand/randn(*sizes) | 均匀/标准分布\n",
    "| normal(mean,std)/uniform(from,to) | 正态分布/均匀分布\n",
    "| randperm(m) | 随机排列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 操作 tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 算术操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0601, 1.4582, 1.2370],\n",
      "        [0.5952, 0.9877, 0.7643],\n",
      "        [1.0322, 0.8298, 0.9940],\n",
      "        [1.1127, 1.0866, 0.7929],\n",
      "        [1.5137, 0.9716, 1.5557]])\n",
      "tensor([[1.0601, 1.4582, 1.2370],\n",
      "        [0.5952, 0.9877, 0.7643],\n",
      "        [1.0322, 0.8298, 0.9940],\n",
      "        [1.1127, 1.0866, 0.7929],\n",
      "        [1.5137, 0.9716, 1.5557]])\n",
      "tensor([[1.0601, 1.4582, 1.2370],\n",
      "        [0.5952, 0.9877, 0.7643],\n",
      "        [1.0322, 0.8298, 0.9940],\n",
      "        [1.1127, 1.0866, 0.7929],\n",
      "        [1.5137, 0.9716, 1.5557]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "y = torch.rand(5, 3)\n",
    "result = torch.empty(5, 3)\n",
    "\n",
    "# 加法形式一\n",
    "print(x + y)\n",
    "# 加法形式二\n",
    "torch.add(x, y, out = result)\n",
    "print(result)\n",
    "# 加法形式三 -> inplace\n",
    "y = y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "索引出来的结果与原数据共享内存，也即修改一个，另一个会跟着修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0820, 0.7458, 0.5723])\n"
     ]
    }
   ],
   "source": [
    "y = x[0, :]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**常用函数**\n",
    "\n",
    "| 函数 | 功能\n",
    "| :-- | :--|\n",
    "| index_select(input, dim, index) | 在指定维度 dim 上选取，比如选取某些行、某些列\n",
    "| masked_select(input, mask) | 例子如上，a[a>0]，使用 ByteTensor 进行选取\n",
    "| nonzero(input) | 非0元素的下标\n",
    "| gather(input, dim, index) | 根据 index，在 dim 维度上选取数据，输出的 size 与 index 一样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 改变形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0820, 0.7458, 0.5723, 0.1330, 0.7487, 0.7195, 0.2965, 0.8292, 0.8502,\n",
      "        0.8069, 0.3472, 0.4902, 0.7324, 0.5613, 0.8777])\n",
      "tensor([0.0820, 0.7458, 0.5723, 0.1330, 0.7487, 0.7195, 0.2965, 0.8292, 0.8502,\n",
      "        0.8069, 0.3472, 0.4902, 0.7324, 0.5613, 0.8777])\n"
     ]
    }
   ],
   "source": [
    "# view 改变形状，也共享内存\n",
    "# 注意 view() 返回的新 Tensor 与源 Tensor 虽然可能有不同的 size，\n",
    "# 但是是共享 data 的，也即更改其中的一个，另外一个也会跟着改变。\n",
    "# (顾名思义，view仅仅是改变了对这个张量的观察角度，内部数据并未改变)\n",
    "y = x.view(15)\n",
    "print(y)\n",
    "\n",
    "# 使用 clone() 可以创建新副本， 不共享 data\n",
    "z = x.clone().view(15)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18004173040390015\n"
     ]
    }
   ],
   "source": [
    "# 函数就是item(), 可以将一个标量 Tensor 转换成一个 Python number\n",
    "z = torch.rand(1)\n",
    "print(z.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 线性代数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 函数 | 功能\n",
    "| :-- | :-- |\n",
    "| trace | 对角线元素之和 ( 矩阵的迹 )\n",
    "| diag | 对角线元素\n",
    "| triu/tril | 矩阵的上三角/下三角，可指定偏移量\n",
    "| mm/bmm | 矩阵乘法，batch 的矩阵乘法\n",
    "| addmm/addbmm/addmv/addr/baddbmm..| 矩阵运算\n",
    "| t | 转置\n",
    "| dot/cross | 内积/外积\n",
    "| inverse | 求逆矩阵\n",
    "| svd | 奇异值分解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 广播机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当对两个形状不同的Tensor按元素运算时，可能会触发广播（broadcasting）机制：先适当复制元素使这两个Tensor形状相同后再按元素运算。也就是自动补充数据的机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[2, 3],\n",
      "        [3, 4],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1, 3).view(1, 2)\n",
    "print(x)\n",
    "y = torch.arange(1, 4).view(3, 1)\n",
    "print(y)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 运算的内存开销"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 索引操作是不会开辟新内存的，而像 y = x + y 这样的运算是会新开内存的，然后将y指向新内存\n",
    "id_before = id(y)\n",
    "y = y + x\n",
    "print(id(y) == id_before) # False \n",
    "\n",
    "# 如果想指定结果到原来的 y 的内存，我们可以使用索引来进行替换操作\n",
    "id_before = id(y)\n",
    "y[:] = y + x\n",
    "print(id(y) == id_before) # True\n",
    "\n",
    "# 还可以使用运算符全名函数中的 out 参数或者 自加运算符+=( 也即add_())达到上述效果\n",
    "id_before = id(y)\n",
    "torch.add(x, y, out=y)\n",
    "print(id(y) == id_before) # True\n",
    "\n",
    "id_before = id(y)\n",
    "y += x\n",
    "print(id(y) == id_before) # True\n",
    "\n",
    "id_before = id(y)\n",
    "y.add_(x)\n",
    "print(id(y) == id_before) # True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5 Tensor 和 Numpy 的互换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- numpy() 将 Tensor 转成 Numpy 数组\n",
    "- from_numpy() 将 Numpy 数组转成 Tensor\n",
    "- 这两个函数所产生的的 Tensor 和 NumPy 中的数组共享相同的内存，改变其中一个时另一个也会改变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1.]\n",
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Tensor -> Numpy\n",
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "# Numpy -> Tensor\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外还有一个常用的方法就是直接用 torch.tensor() 将 NumPy 数组转换成 Tensor，需要注意的是该方法总是会进行数据拷贝，返回的 Tensor 和原来的数据不再共享内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.] tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "c = torch.tensor(a)\n",
    "a += 1\n",
    "print(a, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.6 Tensor On GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用方法 to() 可以将 Tensor 在 CPU 和 GPU 之间相互移动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 3]], device='cuda:0')\n",
      "tensor([[2., 3.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 以下代码只有在PyTorch GPU版本上才会执行\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # GPU\n",
    "    y = torch.ones_like(x, device=device)  # 直接创建一个在 GPU 上的 Tensor\n",
    "    x = x.to(device)                       # 等价于 .to(\"cuda\")\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # to()还可以同时更改数据类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 自动求梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch 提供的 autograd 包能够根据输入和**前向传播**过程自动构建计算图，并执行**反向传播**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 概念\n",
    "\n",
    "1. 求梯度\n",
    "\n",
    "- 将 Tensor 属性 .requires_grad 设置为 True，将开始追踪在其上的所有操作（这样就可以利用链式法则进行梯度传播了）\n",
    "- 调用 .backward() 完成所有梯度计算， Tensor 的梯度将累积到 .grad 属性中\n",
    "- .backward()时，如果 y 是标量，则不需要为 backward() 传入任何参数；否则，需要传入一个与 y 同形的 Tensor\n",
    "\n",
    "2. 取消梯度\n",
    "\n",
    "- 调用 .detach(), 取消跟踪，防止将来的计算被追踪\n",
    "- 可用 with torch.no_grad() 将不想被追踪的操作代码块包裹起来，这种方法在评估模型的时候很常用。\n",
    "- 也可 设置 requires_grad = False 取消梯度。\n",
    "\n",
    "3. 计算函数\n",
    "- Tensor 和 Function 互相结合就可构建一个记录有整个计算过程的有向无环图（DAG）。\n",
    "- Tensor 有一个 .grad_fn 属性，该属性即创建该 Tensor 的 Function。\n",
    "- 就是说该 Tensor 是不是通过某些运算得到的，若是，则 grad_fn 返回一个与这些运算相关的对象，否则是 None。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置 requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "None\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x7f35e405d430>\n"
     ]
    }
   ],
   "source": [
    "# 设置 requires_grad\n",
    "x = torch.ones(2, 2, requires_grad = True)\n",
    "print(x)\n",
    "print(x.grad_fn)\n",
    "\n",
    "y = x + 2\n",
    "print(y)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过 .requires_grad_() 来用 in-place 的方式改变 requires_grad 属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x7f34f3788940>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2) # 缺失情况下默认 requires_grad = False\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad) # False\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad) # True\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 y.backward() 时，如果 y 是标量，则不需要为 backward() 传入任何参数；否则，需要传入一个与 y 同形的 Tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5000, 0.5000, 0.5000, 0.5000])\n",
      "tensor([2.5000, 0.7000, 0.5200, 0.5020])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)\n",
    "y = 2 * x\n",
    "# m 是标量， backward() 不传入任何参数\n",
    "m = y.mean()\n",
    "m.backward()\n",
    "print(x.grad)\n",
    "\n",
    "y = 2 * x\n",
    "# z 不是是标量， backward() 需要传入 y 同形的 Tensor\n",
    "z = y.view(2, 2)\n",
    "v = torch.tensor([[1.0, 0.1], [0.01, 0.001]], dtype=torch.float)\n",
    "z.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grad 在反向传播过程中是累加的，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以一般在反向传播之前需把梯度清零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.5000, 1.7000, 1.5200, 1.5020])\n",
      "tensor([1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# 再来反向传播一次，注意grad是累加的\n",
    "out2 = x.sum()\n",
    "out2.backward()\n",
    "print(x.grad)\n",
    "\n",
    "out3 = x.sum()\n",
    "# grad 一定要清零\n",
    "x.grad.data.zero_()\n",
    "out3.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中断梯度追踪的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor(1., grad_fn=<PowBackward0>) True\n",
      "tensor(1.) False\n",
      "tensor(2., grad_fn=<AddBackward0>) True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y1 = x ** 2\n",
    "# 取消求梯度\n",
    "with torch.no_grad():\n",
    "    y2 = x ** 3\n",
    "y3 = y1 + y2\n",
    "\n",
    "print(x.requires_grad)\n",
    "print(y1, y1.requires_grad) # True\n",
    "print(y2, y2.requires_grad) # Falsebb\n",
    "print(y3, y3.requires_grad) # True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么是 $y_3$ 的导数是 2 呢？$y_3 = y_1 + y_2 = x^2 + x^3$，当 $x = 1$ 时 $\\frac{dy_3}{dx}$ 不应该是 5 吗？事实上，由于 $y_2$ 的定义是被torch.no_grad():包裹的，所以与 $y_2$ 有关的梯度是不会回传的，只有与 $y_1$ 有关的梯度才会回传，即 $x_2$ 对 $x$ 的梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们想要修改 tensor 的数值，但是又不希望被 autograd 记录（即不会影响反向传播），那么我么可以对 tensor.data 进行操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n",
      "False\n",
      "tensor([100.], requires_grad=True)\n",
      "tensor([200.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1, requires_grad=True)\n",
    "\n",
    "print(x.data) # 还是一个tensor\n",
    "print(x.data.requires_grad) # 但是已经是独立于计算图之外\n",
    "\n",
    "y = x * x\n",
    "x.data *= 100 # 只改变了值，不会记录在计算图，所以不会影响梯度传播\n",
    "\n",
    "y.backward()\n",
    "print(x) # 更改data的值也会影响tensor的值\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
