{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 环境配置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在开始学习 pytorch 之前， 我们先来学习一下运行 pytorch 的环境，以及 pytorch 的基本用法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Anaconda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anaconda 是什么？\n",
    "\n",
    "Anaconda 是 Python 的一个开源发行版本，主要面向科学计算。我们可以这样简单地理解，Anaconda是一个预装了很多用到的或用不到的第三方库的 Python。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anaconda 安装\n",
    "\n",
    "- 第一步： 官网下载 (https://www.anaconda.com)\n",
    "- 第二步： 安装软件\n",
    "- 第三步： 测试安装是否成功\n",
    "\n",
    "```\n",
    "检测下列软件是否可以正常执行\n",
    "$ python\n",
    "$ ipython\n",
    "$ conda\n",
    "$ jupyter notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anaconda 环境管理 \n",
    "\n",
    "```\n",
    "$ conda create -name env_name           # 创建新的环境\n",
    "$ conda activate env_name               # 激活环境\n",
    "$ conda deactivate                      # 退出环境\n",
    "$ conda remove env_name --all           # 删除环境\n",
    "```\n",
    "\n",
    "#### Anaconda 软件包管理 \n",
    "\n",
    "```\n",
    "$ conda list                                  # 查看所有已安装的包\n",
    "$ conda install package_name                  # 当前环境下安装包\n",
    "$ conda remove package_name                   # 删除当前环境下包\n",
    "$ conda install -name env_name package_name   # 指定环境下安装包\n",
    "$ conda remove -name env_name package_name    # 删除指定环境下安装包\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 常见问题\n",
    "\n",
    "1. 下载 anaconda 的速度太慢，怎么办？\n",
    "\n",
    "    可以使用国内清华大学提供的镜像站进行下载。 镜像站的地址为 `https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/`\n",
    "    \n",
    "\n",
    "2. 使用 conda 命令安装软件太慢，怎么办？\n",
    "\n",
    "   可以更换 conda 清华大学的镜像源，操作如下：\n",
    "\n",
    "```\n",
    "conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\n",
    "conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/\n",
    "conda config --set show_channel_urls yes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### jupyter 是什么\n",
    "\n",
    "Jupyter notebook（http://jupyter.org/） 是一种 Web 应用，能让用户将说明文本、数学方程、代码和可视化内容全部组合到一个易于共享的文档中。\n",
    "\n",
    "#### 安装使用\n",
    "\n",
    "- 第一步： 安装 anaconda\n",
    "- 第二步： 命令行输入 jupyter note\n",
    "- 第三步： 浏览器打开 http://localhost:8888 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ####  安装\n",
    " \n",
    " - 安装 gpu 版\n",
    " \n",
    " conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch\n",
    " \n",
    " 注：cudatoolkit 是指定机器的 cuda 版本，这个需要根据机器的情况填写不同的版本信息\n",
    " \n",
    " \n",
    " - 安装 cpu 版\n",
    " \n",
    " conda install pytorch torchvision torchaudio cpuonly -c pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 pytorch 数据操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 创建 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建 初始 5x3 矩阵\n",
    "x = torch.empty(5, 3)\n",
    "\n",
    "# 创建 随机 5x3 矩阵\n",
    "x = torch.rand(5, 3)\n",
    "\n",
    "# 创建 long 型 5x3 零矩形\n",
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "\n",
    "# 根据数据创建 Tensor\n",
    "x = torch.tensor([5.01, 6.])\n",
    "\n",
    "# 返回的tensor默认具有相同的 torch.dtype和torch.device\n",
    "x = x.new_ones(5, 3, dtype=torch.float64)  \n",
    "\n",
    "# 指定新的数据类型\n",
    "x = torch.randn_like(x, dtype=torch.float) \n",
    "\n",
    "# 获取数据形状\n",
    "print(x.size())\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**常用 函数**\n",
    "\n",
    "| 函数 |功能\n",
    "| :-- | :--\n",
    "| Tensor(*sizes) | 基础构造函数\n",
    "| tensor(data) | 数据类型转换为 tensor\n",
    "| ones(*sizes)\t| 全 1 Tensor\n",
    "| zeros(*sizes)\t| 全 0 Tensor\n",
    "| eye(*sizes)\t| 对角线为 1，其他为 0\n",
    "| arange(s,e,step) | 从 s 到 e，步长为 step\n",
    "| linspace(s,e,steps) | 从 s 到 e，均匀切分成 steps 份\n",
    "| rand/randn(*sizes) | 均匀/标准分布\n",
    "| normal(mean,std)/uniform(from,to) | 正态分布/均匀分布\n",
    "| randperm(m) | 随机排列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 操作 tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 算术操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5, 3)\n",
    "y = torch.rand(5, 3)\n",
    "result = torch.empty(5, 3)\n",
    "\n",
    "# 加法形式一\n",
    "x + y\n",
    "\n",
    "# 加法形式二\n",
    "torch.add(x, y, out = result)\n",
    "\n",
    "# 加法形式三 -> inplace\n",
    "y = y.add_(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "索引出来的结果与原数据共享内存，也即修改一个，另一个会跟着修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**常用函数**\n",
    "\n",
    "| 函数 | 功能\n",
    "| :-- | :--|\n",
    "| index_select(input, dim, index) | 在指定维度 dim 上选取，比如选取某些行、某些列\n",
    "| masked_select(input, mask) | 例子如上，a[a>0]，使用 ByteTensor 进行选取\n",
    "| nonzero(input) | 非0元素的下标\n",
    "| gather(input, dim, index) | 根据 index，在 dim 维度上选取数据，输出的 size 与 index 一样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 改变形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view 改变形状，也共享内存\n",
    "# 注意 view() 返回的新 Tensor 与源 Tensor 虽然可能有不同的 size，\n",
    "# 但是是共享 data 的，也即更改其中的一个，另外一个也会跟着改变。\n",
    "# (顾名思义，view仅仅是改变了对这个张量的观察角度，内部数据并未改变)\n",
    "y = x.view(15)\n",
    "\n",
    "# 使用 clone() 可以创建新副本， 不共享 data\n",
    "z = x.clone().view(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8247038722038269"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 函数就是item(), 可以将一个标量 Tensor 转换成一个 Python number\n",
    "z = torch.rand(1)\n",
    "z.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 线性代数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 函数 | 功能\n",
    "| :-- | :-- |\n",
    "| trace | 对角线元素之和 ( 矩阵的迹 )\n",
    "| diag | 对角线元素\n",
    "| triu | 矩阵的上三角，可指定偏移量\n",
    "| tril | 矩阵的下三角，可指定偏移量\n",
    "| mm | 矩阵乘法\n",
    "| bmm | batch 的矩阵乘法\n",
    "| addmm/addbmm/addmv/addr/baddbmm..| 矩阵运算\n",
    "| t | 转置\n",
    "| dot/cross | 内积/外积\n",
    "| inverse | 求逆矩阵\n",
    "| svd | 奇异值分解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 广播机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当对两个形状不同的 Tensor 按元素运算时，可能会触发广播（broadcasting）机制：先适当复制元素使这两个 Tensor 形状相同后再按元素运算。也就是自动补充数据的机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 3],\n",
       "        [3, 4],\n",
       "        [4, 5]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(1, 3).view(1, 2)\n",
    "y = torch.arange(1, 4).view(3, 1)\n",
    "x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 运算的内存开销"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 索引操作是不会开辟新内存的，而像 y = x + y 这样的运算是会新开内存的，然后将y指向新内存\n",
    "id_before = id(y)\n",
    "y = y + x\n",
    "print(id(y) == id_before) # False \n",
    "\n",
    "# 如果想指定结果到原来的 y 的内存，我们可以使用索引来进行替换操作\n",
    "id_before = id(y)\n",
    "y[:] = y + x\n",
    "print(id(y) == id_before) # True\n",
    "\n",
    "# 还可以使用运算符全名函数中的 out 参数或者 自加运算符+=( 也即add_())达到上述效果\n",
    "id_before = id(y)\n",
    "torch.add(x, y, out=y)\n",
    "print(id(y) == id_before) # True\n",
    "\n",
    "id_before = id(y)\n",
    "y += x\n",
    "print(id(y) == id_before) # True\n",
    "\n",
    "id_before = id(y)\n",
    "y.add_(x)\n",
    "print(id(y) == id_before) # True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5 Tensor 和 Numpy 的互换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- numpy() 将 Tensor 转成 Numpy 数组\n",
    "- from_numpy() 将 Numpy 数组转成 Tensor\n",
    "\n",
    "这两个函数所产生的的 Tensor 和 Numpy 中的数组共享相同的内存，改变其中一个时另一个也会改变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Tensor -> Numpy\n",
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "\n",
    "# Numpy -> Tensor\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外还有一个常用的方法就是直接用 torch.tensor() 将 Numpy 数组转换成 Tensor，需要注意的是该方法总是会进行数据拷贝，返回的 Tensor 和原来的数据不再共享内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.tensor(a)\n",
    "a += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.6 Tensor On GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用方法 to() 可以将 Tensor 在 CPU 和 GPU 之间相互移动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下代码只有在PyTorch GPU版本上才会执行\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # GPU\n",
    "    y = torch.ones_like(x, device=device)  # 直接创建一个在 GPU 上的 Tensor\n",
    "    x = x.to(device)                       # 等价于 .to(\"cuda\")\n",
    "    z = x + y\n",
    "    z.to(\"cpu\", torch.double)              # to()还可以同时更改数据类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 自动求梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch 提供的 autograd 包能够根据输入和**前向传播**过程自动构建计算图，并执行**反向传播**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 概念\n",
    "\n",
    "1. 求梯度\n",
    "\n",
    "- 将 Tensor 属性 .requires_grad 设置为 True，将开始追踪在其上的所有操作（这样就可以利用链式法则进行梯度传播了）\n",
    "- 调用 .backward() 完成所有梯度计算， Tensor 的梯度将累积到 .grad 属性中\n",
    "- .backward() 时，如果 y 是标量，则不需要为 backward() 传入任何参数；否则，需要传入一个与 y 同形的 Tensor\n",
    "\n",
    "2. 取消梯度\n",
    "\n",
    "- 调用 .detach(), 取消跟踪，防止将来的计算被追踪\n",
    "- 可用 with torch.no_grad() 将不想被追踪的操作代码块包裹起来，这种方法在评估模型的时候很常用。\n",
    "- 也可 设置 requires_grad = False 取消梯度。\n",
    "\n",
    "3. 计算函数\n",
    "- Tensor 和 Function 互相结合就可构建一个记录有整个计算过程的有向无环图（DAG）。\n",
    "- Tensor 有一个 .grad_fn 属性，该属性即创建该 Tensor 的 Function。\n",
    "- 就是说该 Tensor 是不是通过某些运算得到的，若是，则 grad_fn 返回一个与这些运算相关的对象，否则是 None。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置 requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<AddBackward0 object at 0x7f78f126a2e0>\n"
     ]
    }
   ],
   "source": [
    "# 设置 requires_grad\n",
    "x = torch.ones(2, 2, requires_grad = True)\n",
    "print(x.grad_fn)\n",
    "\n",
    "y = x + 2\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过 .requires_grad_() 来用 in-place 的方式改变 requires_grad 属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x7f78f1276550>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2) # 缺失情况下默认 requires_grad = False\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad) # False\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad) # True\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 y.backward() 时，如果 y 是标量，则不需要为 backward() 传入任何参数；否则，需要传入一个与 y 同形的 Tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5000, 0.5000, 0.5000, 0.5000])\n",
      "tensor([2.5000, 0.7000, 0.5200, 0.5020])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)\n",
    "y = 2 * x\n",
    "# m 是标量， backward() 不传入任何参数\n",
    "m = y.mean()\n",
    "m.backward()\n",
    "print(x.grad)\n",
    "\n",
    "y = 2 * x\n",
    "# z 不是是标量， backward() 需要传入 y 同形的 Tensor\n",
    "z = y.view(2, 2)\n",
    "v = torch.tensor([[1.0, 0.1], [0.01, 0.001]], dtype=torch.float)\n",
    "z.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grad 在反向传播过程中是累加的，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以一般在反向传播之前需把梯度清零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.5000, 1.7000, 1.5200, 1.5020])\n",
      "tensor([1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# 再来反向传播一次，注意grad是累加的\n",
    "out2 = x.sum()\n",
    "out2.backward()\n",
    "print(x.grad)\n",
    "\n",
    "out3 = x.sum()\n",
    "# grad 一定要清零\n",
    "x.grad.data.zero_()\n",
    "out3.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中断梯度追踪的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor(1., grad_fn=<PowBackward0>) True\n",
      "tensor(1.) False\n",
      "tensor(2., grad_fn=<AddBackward0>) True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y1 = x ** 2\n",
    "# 取消求梯度\n",
    "with torch.no_grad():\n",
    "    y2 = x ** 3\n",
    "y3 = y1 + y2\n",
    "\n",
    "print(x.requires_grad)\n",
    "print(y1, y1.requires_grad) # True\n",
    "print(y2, y2.requires_grad) # Falsebb\n",
    "print(y3, y3.requires_grad) # True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么是 $y_3$ 的导数是 2 呢？$y_3 = y_1 + y_2 = x^2 + x^3$，当 $x = 1$ 时 $\\frac{dy_3}{dx}$ 不应该是 5 吗？事实上，由于 $y_2$ 的定义是被 torch.no_grad() 包裹的，所以与 $y_2$ 有关的梯度是不会回传的，只有与 $y_1$ 有关的梯度才会回传，即 $x_2$ 对 $x$ 的梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们想要修改 tensor 的数值，但是又不希望被 autograd 记录（即不会影响反向传播），那么我么可以对 tensor.data 进行操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n",
      "False\n",
      "tensor([100.], requires_grad=True)\n",
      "tensor([200.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1, requires_grad=True)\n",
    "\n",
    "print(x.data) # 还是一个tensor\n",
    "print(x.data.requires_grad) # 但是已经是独立于计算图之外\n",
    "\n",
    "y = x * x\n",
    "x.data *= 100 # 只改变了值，不会记录在计算图，所以不会影响梯度传播\n",
    "\n",
    "y.backward()\n",
    "print(x) # 更改data的值也会影响tensor的值\n",
    "print(x.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
